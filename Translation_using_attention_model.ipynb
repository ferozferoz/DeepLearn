{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation_using_attention_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferozferoz/DeepLearn/blob/master/Translation_using_attention_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otvQnRnxjisu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q faker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TitCGeZMjADP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model, Model\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n",
        "\n",
        "# change this if you want it to work with another language\n",
        "LOCALES = ['en_US']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c90hCpdye4JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates \n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt\n",
        "\n",
        "def load_dataset(m):\n",
        "    \"\"\"\n",
        "        Loads a dataset with m examples and vocabularies\n",
        "        :m: the number of examples to generate\n",
        "    \"\"\"\n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    \n",
        "    for i in tqdm(range(m)):\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(m))\n",
        "    \n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'],list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        "    return dataset, human, machine, inv_machine\n",
        "\n",
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "    \n",
        "    X, Y = zip(*dataset)\n",
        "    print(X[0])\n",
        "    print(Y[0])\n",
        "    \n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
        "    \n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "    return X, np.array(Y), Xoh, Yoh\n",
        "\n",
        "def string_to_int(string, length, vocab):\n",
        "    \"\"\"\n",
        "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
        "    input string's characters in the \"vocab\"\n",
        "    \n",
        "    Arguments:\n",
        "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
        "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
        "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
        "    \n",
        "    Returns:\n",
        "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
        "    \"\"\"\n",
        "    \n",
        "    #make lower to standardize\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "    \n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "        \n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "    \n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "    \n",
        "    #print (rep)\n",
        "    return rep\n",
        "\n",
        "\n",
        "def int_to_string(ints, inv_vocab):\n",
        "    \"\"\"\n",
        "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
        "    \n",
        "    Arguments:\n",
        "    ints -- list of integers representing indexes in the machine's vocabulary\n",
        "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n",
        "    \n",
        "    Returns:\n",
        "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
        "    \"\"\"\n",
        "    \n",
        "    l = [inv_vocab[i] for i in ints]\n",
        "    return l\n",
        "\n",
        "\n",
        "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iEuWKklj4hE",
        "colab_type": "code",
        "outputId": "39d1bf53-78c4-4766-86a9-8e9c95068ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 15919.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yNpvGgOj67t",
        "colab_type": "code",
        "outputId": "8daa42f6-20e6-47d7-d8a1-aa5ae0d924e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "dataset[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('9 may 1998', '1998-05-09'),\n",
              " ('10.11.19', '2019-11-10'),\n",
              " ('9/10/70', '1970-09-10'),\n",
              " ('saturday april 28 1990', '1990-04-28'),\n",
              " ('thursday january 26 1995', '1995-01-26'),\n",
              " ('monday march 7 1983', '1983-03-07'),\n",
              " ('sunday may 22 1988', '1988-05-22'),\n",
              " ('08 jul 2008', '2008-07-08'),\n",
              " ('8 sep 1999', '1999-09-08'),\n",
              " ('thursday january 1 1981', '1981-01-01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "111W756aj-V8",
        "colab_type": "code",
        "outputId": "fd4cd91a-52ee-4527-876b-9779cb64d699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "print(\"Human vocab is a dictionary that maps all possible characters used in writing date to a machine number\")\n",
        "print(\"***********************************\")\n",
        "human_vocab"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Human vocab is a dictionary that maps all possible characters used in writing date to a machine number\n",
            "***********************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '.': 1,\n",
              " '/': 2,\n",
              " '0': 3,\n",
              " '1': 4,\n",
              " '2': 5,\n",
              " '3': 6,\n",
              " '4': 7,\n",
              " '5': 8,\n",
              " '6': 9,\n",
              " '7': 10,\n",
              " '8': 11,\n",
              " '9': 12,\n",
              " '<pad>': 36,\n",
              " '<unk>': 35,\n",
              " 'a': 13,\n",
              " 'b': 14,\n",
              " 'c': 15,\n",
              " 'd': 16,\n",
              " 'e': 17,\n",
              " 'f': 18,\n",
              " 'g': 19,\n",
              " 'h': 20,\n",
              " 'i': 21,\n",
              " 'j': 22,\n",
              " 'l': 23,\n",
              " 'm': 24,\n",
              " 'n': 25,\n",
              " 'o': 26,\n",
              " 'p': 27,\n",
              " 'r': 28,\n",
              " 's': 29,\n",
              " 't': 30,\n",
              " 'u': 31,\n",
              " 'v': 32,\n",
              " 'w': 33,\n",
              " 'y': 34}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snvo1j2SkjVk",
        "colab_type": "code",
        "outputId": "8c5d809d-d2fe-4f84-9422-6c1bc2a496a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "machine_vocab"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-': 0,\n",
              " '0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dOkpHS3krnI",
        "colab_type": "code",
        "outputId": "235622b3-82c5-43ff-f8ce-0e09065371a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "inv_machine_vocab"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-',\n",
              " 1: '0',\n",
              " 2: '1',\n",
              " 3: '2',\n",
              " 4: '3',\n",
              " 5: '4',\n",
              " 6: '5',\n",
              " 7: '6',\n",
              " 8: '7',\n",
              " 9: '8',\n",
              " 10: '9'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US2W3X8oc4uv",
        "colab_type": "code",
        "outputId": "c832fca9-a74d-4628-dfc4-518c07d935f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 may 1998\n",
            "1998-05-09\n",
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osLVGW9E1TRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1e2dd55-7256-411d-c703-3beb9af0ebce"
      },
      "source": [
        "\"\"\" dataset at location 1\"\"\"\n",
        "dataset[1]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('10.11.19', '2019-11-10')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR711vBj0wK5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "3f2fa4b4-7958-4927-9098-a3db2e5f862e"
      },
      "source": [
        "\"\"\" 10.11.19 is encoded in 8 places, rest are padded spaces. Encodings are done correctly\"\"\"\n",
        "Xoh[1][1:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnJqxfpG19U5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "f0cbe09e-18c7-436d-ad32-b7faacbc825f"
      },
      "source": [
        "Yoh[1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6ABr4D0pxL7",
        "colab_type": "code",
        "outputId": "d512b089-82eb-4503-e475-f4034b0991b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor = Dense(1, activation = \"relu\")\n",
        "activator = Activation('softmax', name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGaN0Y4Vqeem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(a, s_prev):   \n",
        "    s_prev = repeator(s_prev)    \n",
        "    concat = concatenator([a, s_prev])    \n",
        "    e = densor(concat)    \n",
        "    alphas = activator(e)    \n",
        "    context = dotor([alphas, a])\n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8XbQ7YFu8ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a = 64\n",
        "n_s = 128\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation='softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbQcH6R7u9yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s,c])\n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(s)\n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
        "    ### END CODE HERE ###\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akmSKwUJvCe5",
        "colab_type": "code",
        "outputId": "182c0327-521e-41b6-c311-98a71edcbb96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPe_cTTpvEqo",
        "colab_type": "code",
        "outputId": "4cb55509-4e53-420f-f456-d1ec0157fd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 128)      52224       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 128)      0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 256)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30, 1)        257         concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 11)           1419        lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 185,484\n",
            "Trainable params: 185,484\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcuvhS4IvXy_",
        "colab_type": "code",
        "outputId": "11bc954f-6863-4a96-9655-00fc62757e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
        "                    metrics=['accuracy'],\n",
        "                    loss='categorical_crossentropy')\n",
        "out"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QApafokveZs",
        "colab_type": "code",
        "outputId": "39cf1ff3-71ef-4f5b-cbf5-7d3b265f4979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))\n",
        "Yoh.swapaxes(0,1).shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10000, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m8Af1CPvhG8",
        "colab_type": "code",
        "outputId": "1d2c64bf-ed75-4b9d-b130-8bb2349b830e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=10, batch_size=100)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "10000/10000 [==============================] - 23s 2ms/step - loss: 12.7932 - dense_2_loss: 2.1842 - dense_2_acc: 0.8003 - dense_2_acc_1: 0.8673 - dense_2_acc_2: 0.6023 - dense_2_acc_3: 0.2310 - dense_2_acc_4: 0.8299 - dense_2_acc_5: 0.5417 - dense_2_acc_6: 0.2099 - dense_2_acc_7: 0.7829 - dense_2_acc_8: 0.3665 - dense_2_acc_9: 0.2226\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 4.9296 - dense_2_loss: 1.3763 - dense_2_acc: 0.9684 - dense_2_acc_1: 0.9724 - dense_2_acc_2: 0.8473 - dense_2_acc_3: 0.5749 - dense_2_acc_4: 0.9999 - dense_2_acc_5: 0.9627 - dense_2_acc_6: 0.6895 - dense_2_acc_7: 0.9997 - dense_2_acc_8: 0.7128 - dense_2_acc_9: 0.4497\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 2.6512 - dense_2_loss: 0.8308 - dense_2_acc: 0.9798 - dense_2_acc_1: 0.9808 - dense_2_acc_2: 0.8843 - dense_2_acc_3: 0.8686 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9761 - dense_2_acc_6: 0.8865 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.8572 - dense_2_acc_9: 0.7181\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 1.5796 - dense_2_loss: 0.4465 - dense_2_acc: 0.9842 - dense_2_acc_1: 0.9848 - dense_2_acc_2: 0.8943 - dense_2_acc_3: 0.9385 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9806 - dense_2_acc_6: 0.9258 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.9100 - dense_2_acc_9: 0.8589\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 1.1727 - dense_2_loss: 0.3193 - dense_2_acc: 0.9866 - dense_2_acc_1: 0.9881 - dense_2_acc_2: 0.9103 - dense_2_acc_3: 0.9643 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9836 - dense_2_acc_6: 0.9412 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.9369 - dense_2_acc_9: 0.8922\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 0.9521 - dense_2_loss: 0.2591 - dense_2_acc: 0.9894 - dense_2_acc_1: 0.9894 - dense_2_acc_2: 0.9334 - dense_2_acc_3: 0.9785 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9864 - dense_2_acc_6: 0.9481 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.9502 - dense_2_acc_9: 0.9090\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 0.7873 - dense_2_loss: 0.2169 - dense_2_acc: 0.9916 - dense_2_acc_1: 0.9911 - dense_2_acc_2: 0.9591 - dense_2_acc_3: 0.9862 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9875 - dense_2_acc_6: 0.9556 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.9589 - dense_2_acc_9: 0.9260\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 0.6598 - dense_2_loss: 0.1887 - dense_2_acc: 0.9938 - dense_2_acc_1: 0.9936 - dense_2_acc_2: 0.9770 - dense_2_acc_3: 0.9920 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9881 - dense_2_acc_6: 0.9583 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.9701 - dense_2_acc_9: 0.9370\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 0.5467 - dense_2_loss: 0.1599 - dense_2_acc: 0.9950 - dense_2_acc_1: 0.9951 - dense_2_acc_2: 0.9862 - dense_2_acc_3: 0.9947 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9900 - dense_2_acc_6: 0.9645 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.9765 - dense_2_acc_9: 0.9515\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 16s 2ms/step - loss: 0.4690 - dense_2_loss: 0.1384 - dense_2_acc: 0.9962 - dense_2_acc_1: 0.9959 - dense_2_acc_2: 0.9906 - dense_2_acc_3: 0.9958 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9924 - dense_2_acc_6: 0.9651 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 0.9816 - dense_2_acc_9: 0.9605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd22f276f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mah3jixwqF6",
        "colab_type": "code",
        "outputId": "5d77a04c-423c-4bd9-bddd-b012237ee665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "\"\"\"demostrating how source date is converted to target dates using attention model\"\"\"\n",
        "\n",
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "\n",
        "for example in EXAMPLES:\n",
        "    \n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
        "    source = source.reshape((1,source.shape[0],source.shape[1]))\n",
        "    prediction = model.predict([source, s0, c0])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source: 3 May 1979\n",
            "output: 1979-05-03\n",
            "source: 5 April 09\n",
            "output: 2009-04-05\n",
            "source: 21th of August 2016\n",
            "output: 2016-08-21\n",
            "source: Tue 10 Jul 2007\n",
            "output: 2007-07-10\n",
            "source: Saturday May 9 2018\n",
            "output: 2018-05-09\n",
            "source: March 3 2001\n",
            "output: 2001-03-03\n",
            "source: March 3rd 2001\n",
            "output: 2011-03-03\n",
            "source: 1 March 2001\n",
            "output: 2001-03-01\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}